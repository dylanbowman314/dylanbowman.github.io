<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Building an RL Environment to Train Agents for Production Debugging</title>
  <link rel="stylesheet" type="text/css" href="index.css" />
  <link rel="stylesheet" type="text/css" href="page.css" />
</head>
<body>
  <nav class="site-nav" aria-label="Site">
    <a href="index.html">Home</a>
    <span class="site-nav-sep">·</span>
    <a href="about.html">About</a>
  </nav>
  <div class="maketitle">
    <h2 class="titleHead">Building an RL Environment to Train Agents for Production Debugging</h2>
    <div class="author"></div><br />
    <div class="date">Last updated 2026-01-02 by Dylan Bowman, Lorenss Martisons, Jay Ram</div>
  </div>
<p><em>At <a href="https://www.hud.ai/">hud</a>, we built an RL environment for ops diagnostics – one that lets agents investigate across Sentry, Supabase, Railway, and Kubernetes. We trained a model on 24 real production tasks and saw a 2x improvement. Now we're releasing the environment publicly.</em></p>
<hr />
<h2>Context</h2>
<p>As engineers at a fast-growing startup, a solid 10-20% of our time is taken up by debugging bugs in production. The way bugs are solved in production is pretty mechanistic:
1. See error on Sentry
2. Check Supabase, Railway and Kubernetes Dashboard
3. Find errors that match with the time of the bug
4. Cross reference github and docs in order to make a patch to the relevant repository</p>
<p>After doing this a few dozen times, we pondered why we couldn't just have an agent do it, or at the very least assist. The reason is simply that it doesn't have access to the environment, and without the correct prompt or RFT the agent won't be able to intuitively fix bugs the way we can.</p>
<p>This resulted in us getting rabbit-holed and creating an agent trained on our production data that can debug Sentry errors.
The naive implementation of giving an LLM access to 104 tools didn't work, so we created an architecture that involved multiple environments accessible via subagents.</p>
<figure class="md-figure"><img src="images/architecture.png" alt="fig. 1: agent architecture" style="max-width: 100%"><figcaption>fig. 1: agent architecture</figcaption></figure>
<h2>The Architecture: Hierarchical Agents</h2>
<p>The insight is simple: don't give one agent all the tools. Instead, create an orchestrator environment where the agent's tools are subagents—a Sentry agent, a Supabase agent, a Kubernetes agent. The orchestrator sees just six tools, one per subagent. Behind those six tools are 104 individual MCP tools across all subagents.</p>
<p>Here's the key insight: each subagent is itself an RL environment. The Sentry subagent has its own scenarios, its own tools, its own reward signal. You can train it independently on Sentry-specific tasks. Same for Supabase, same for Kubernetes. Once each subagent is trained, you compose them into the orchestrator environment.</p>
<p>Train the subagents first. Then train the orchestrator.</p>
<h2>The RL Environment</h2>
<p>We're releasing this architecture as a public HUD environment called <a href="https://www.hud.ai/environments/a959e403-6e07-4969-afa6-5db637aefc75">cross-service-diagnostics</a> (<a href="https://github.com/hud-evals/hud-ops-diagnostics">GitHub</a>). Plug in your production API keys—your Sentry token, your Supabase credentials, whatever services you use—and you have an ops diagnostics agent for your stack. Fork it, modify it, train on it.</p>
<p>But an environment alone isn't enough to train an agent: we need to generate tasks. We started with the Sentry subagent.</p>
<h2>Training the Sentry Subagent: 24 Real Tasks</h2>
<p>To train the Sentry subagent, we sourced 24 tasks from our actual Sentry instance—real issues from our production systems across different services, error types, and severity levels. Schema validation failures, rate limiting, auth token expiration, WebSocket disconnects, billing edge cases. The diversity matters for generalization.</p>
<figure class="md-figure"><img src="images/sample_rl_tasks.png" alt="fig. 2: 4 sample tasks" style="max-width: 100%"><figcaption>fig. 2: 4 sample tasks</figcaption></figure>
<p>Each task has a verification criterion – specific facts the agent must surface (like an issue ID, a team UUID, or a specific error message) and facts it must not confuse with similar issues. Binary verification: did the agent find the exact right needle in a very large haystack?
The answers come from real production data. Task #0010 expects the agent to find that the user was passing toolu_01XArLykPgwrg24DR3WQJ3Mu – a Claude tool call ID – instead of a trace UUID. Task #0016 expects it to find the function print_hello.</p>
<h2>Training the Sentry Subagent</h2>
<p>With 24 verifiable tasks and an environment, we can run reinforcement learning. Even a small dataset, if diverse enough, can meaningfully optimize a subagent – though a single environment can scale to 500 tasks or more. On HUD, you go to Models, fork a base model (we used o4-mini), then click Train. Point it at your taskset and environment. The platform handles the rest—running rollouts, collecting trajectories, and sending them to the RL backend for training (see the training docs).</p>
<p>HUD supports two training backends: OpenAI RFT (o4-mini) and Tinker (Qwen3 235B, Deepseek V3.1, Kimi K2, and more). Each training run creates a versioned checkpoint on your model, so you can track results and compare across runs.</p>
<h2>Results</h2>
<p>We trained using OpenAI RFT with o4-mini. Training took around 13 hours and ran through 3,000+ traces.
At 15 steps max per scenario, the trained model sentry-o4-mini performs 2x better than base o4-mini (13% vs 6.3%) on our harder Sentry tasks, and beats Gemini 3 Pro and both Claude models—in fewer steps.</p>
<figure class="md-figure"><img src="images/taskset_view.png" alt="fig 3. Taskset view on hud.ai for our internal benchmark" style="max-width: 100%"><figcaption>fig 3. Taskset view on hud.ai for our internal benchmark</figcaption></figure>
<p>This pattern—training on domain-specific tasks to create fast, specialized tools—has improved performance across our other projects too: deep research agents, coding assistants, bug investigation. More case studies coming soon.</p>
<h2>Designing RL Environments That Generalize</h2>
<p>This environment teaches us principles that apply beyond ops diagnostics – to any RL environment for tool-using agents:
Pick a domain with verifiable outcomes. Some examples:
1. Finance works because spreadsheet cells are either correct or not.
    - Support works because tickets get resolved or they don't.
    - Debugging works because you can check if the agent found the right issue.
2. Build from real problems. Go through your actual production failures, customer tickets, or historical tasks. Your real production has quirks – weird error messages, confusing duplicate issues, that cron job someone named print_hello. Train on that.
3. Make verification automatic. If a human has to judge every response, you can't scale. Design tasks where correctness is checkable – specific facts, specific outputs, specific states. LLM-as-judge works for fuzzier domains, but binary verification is cleaner when you can get it.
4. Hierarchical beats flat. Give the agent 6 subagents instead of 104 tools. Each subagent is itself an RL environment you can train independently. Train the subagents on domain-specific tasks, then compose them.
5. Use RL, not just evals. Run rollouts, collect successful trajectories, fine-tune, repeat. The environment becomes a flywheel for improvement.</p>
<h2>Try It Yourself</h2>
<p>We're releasing this RL environment publicly. You can explore the scenarios, connect your own MCP servers, and run diagnostics against your own production stack.</p>
<ul>
<li><a href="https://hud.ai/environments/a959e403-6e07-4969-afa6-5db637aefc75">cross-service-diagnostics environment</a> – scenarios, tools, and ways to integrate</li>
<li><a href="https://docs.hud.ai/cookbooks/ops-diagnostics">SDK cookbook</a> – implementation details and code examples</li>
<li><a href="https://docs.hud.ai/platform/environments">Environment guide</a> – how to build your own RL environments</li>
</ul>
<p>Every trace on the platform captures the full trajectory – actions, observations, tool calls, and reasoning. You can replay exactly how the agent investigated each issue.</p>
<h2>Work with Us</h2>
<p>If you're building agents for production workloads, we can help. HUD provides the infrastructure for reproducible evals, trajectory collection, and model training. We've done this for enterprise spreadsheet work (SheetBench), computer-use agents (OSWorld), and RL environments for DevOps, coding, security, finance, legal document review, medical software, hardware verification, and more.
Reach out to founders@hud.ai or book a call.</p>
  <!-- Lightbox modal -->
  <div class="lightbox-overlay" id="lightbox">
    <img src="" alt="" id="lightbox-img">
  </div>
  <script>
    (function() {
      const overlay = document.getElementById('lightbox');
      const lightboxImg = document.getElementById('lightbox-img');

      // Open lightbox on image click
      document.querySelectorAll('figure.md-figure img, p > img, p > a > img').forEach(img => {
        img.addEventListener('click', function(e) {
          e.preventDefault();
          e.stopPropagation();
          lightboxImg.src = this.src;
          lightboxImg.alt = this.alt;
          overlay.classList.add('active');
          document.body.style.overflow = 'hidden';
        });
      });

      // Close on overlay click
      overlay.addEventListener('click', function(e) {
        if (e.target !== lightboxImg) {
          overlay.classList.remove('active');
          document.body.style.overflow = '';
        }
      });

      // Close on Escape key
      document.addEventListener('keydown', function(e) {
        if (e.key === 'Escape' && overlay.classList.contains('active')) {
          overlay.classList.remove('active');
          document.body.style.overflow = '';
        }
      });
    })();
  </script>
</body>
</html>
